{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Radiantloom Support Assist 7B"
      ],
      "metadata": {
        "id": "zwKAnL4p3yu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install upgrade pip\n",
        "!pip install -q transformers peft accelerate bitsandbytes safetensors sentencepiece langchain gradio sentence-transformers chromadb"
      ],
      "metadata": {
        "id": "IMZxKUwtvzQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # configure Langsmith - it is optional but recommended (get the API key from here - https://smith.langchain.com)\n",
        "# import os\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"radiantloom-support-assist\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"{ADD YOUR LANGSMITH API KEY}\"\n",
        "# os.environ[\"LANGCHAIN_ENDPOINT\"] =\"https://api.smith.langchain.com\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ],
      "metadata": {
        "id": "nRD-1dYE6Cy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
      ],
      "metadata": {
        "id": "T8Lh6avy-tMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model with 4bit quantization\n",
        "model_name = \"Radiantloom/radiantloom-support-assist-7b\"\n",
        "\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1\n",
        "    return tokenizer\n",
        "\n",
        "# load model\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# define stop token ids\n",
        "stop_token_ids = [0]"
      ],
      "metadata": {
        "id": "fEg-xPX1-u0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifiy the prompt\n",
        "prompt = \"\"\"\n",
        "<|im_start|>system\n",
        "You work as a support assistant for AI Geek Labs, an AI research and development lab, handling inquiries and providing assistance through their website.\n",
        "\n",
        "Use the following pieces of context to answer the user's questions. If you don't know the answer, don't make up a answer, just say you don't know.\n",
        "\n",
        "<context>\n",
        "\n",
        "Get AI-powered, Startup-fast: Build & Tune Your LLM MVPs with AI Geek Labs\n",
        "Invent the future of your business with our cutting-edge AI research, development, and fine-tuning services. We're your AI playground.\n",
        "Stop waiting, build! We turn your AI vision into reality, fast. From email assistants to code generation, we build MVPs with the latest AI frameworks \\\n",
        "like Langchain and OpenAI, helping small businesses and startups jumpstart their AI journey.\n",
        "MVP: Turn your AI dream into a working prototype, lightning-fast. We design, develop, and deploy AI-powered MVPs using cutting-edge frameworks \\\n",
        "like Langchain, HuggingFace, and Llama-Index.\n",
        "LLM Fine-tuning: Tap into the true potential of LLMs for your specific needs. We fine-tune models for email assistants, customer support chatbots, \\\n",
        "code generation, text-to-SQL, and more.\n",
        "Open Source Radiance: We're passionate about sharing! Explore our Radiantloom series, featuring open-source LLM models fine-tuned by our experts, \\\n",
        "ready for you to build upon.\n",
        "\n",
        "<context/><|im_end|>\n",
        "<|im_start|>user\n",
        "Question: Which AI frameworks does AI Geek Labs use to build MVPs for clients?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zeKofdLa_JJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=4096,\n",
        "    temperature=0.1, repetition_penalty=1.1, top_p=0.7, top_k=50)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "index_of_assistant = output_text.find(\"<|im_start|>assistant\")\n",
        "result = output_text[index_of_assistant + len(\"<|im_start|>assistant\"):].strip()\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "tv8_OPiY-7W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Implementation"
      ],
      "metadata": {
        "id": "9V6l5TH2RVgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "s4tNGrpgRXKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "dncH_5bjjbXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = HuggingFaceEmbeddings()\n",
        "persist_dir = \"db\"\n",
        "\n",
        "# the websites for the text you want to load\n",
        "websites = [\n",
        "    \"https://www.lodgify.com/airbnb-management-software/\",\n",
        "    \"https://www.lodgify.com/vacation-rental-booking-system/\",\n",
        "    \"https://www.lodgify.com/pricing/\",\n",
        "    \"https://www.lodgify.com/property-management-software/\",\n",
        "    \"https://www.lodgify.com/vacation-rental-website-builder/\",\n",
        "\n",
        "    ]\n",
        "\n",
        "# use the WebBaseLoader to create Document objects for each webside\n",
        "web_loader = WebBaseLoader(websites)\n",
        "documents = web_loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# embed and store the texts\n",
        "# supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = persist_dir\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=texts, embedding=embedding, persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "vectordb.persist()\n",
        "\n",
        "# specify the retriever\n",
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "ResC7vwWiaQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build huggingface pipeline\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_new_tokens=2048,\n",
        "        do_sample=True,\n",
        "        temperature=0.1,\n",
        "        top_k=10,\n",
        "        top_p = 0.95,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "SuSGe6tdoyGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### defining the retrieval chain\n",
        "\n",
        "# specify the llm\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "template = \"\"\"\n",
        "<|im_start|>system\n",
        "You work as a support assistant for Lodgify, a vacation rental property management software platform, handling inquiries and providing assistance through their website.\n",
        "Use the following pieces of context to answer the user's questions. \\\n",
        "If you don't know the answer, don't make up an answer, just say you don't know.\n",
        "\n",
        "<context>\n",
        "    {context}\n",
        "<context/><|im_end|>\n",
        "<|im_start|>user\n",
        "Question: {question}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "def qabot(query):\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever = retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\n",
        "        \"prompt\": PromptTemplate(\n",
        "            template=template,\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "        ),\n",
        "    },\n",
        "    )\n",
        "\n",
        "    results = qa(query)\n",
        "    return results"
      ],
      "metadata": {
        "id": "QbJ3P5CHDTdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# launch gradio UI for inference\n",
        "\n",
        "import gradio as gr\n",
        "import json\n",
        "\n",
        "def support_assist(text):\n",
        "    question = text\n",
        "    response = qabot(question)\n",
        "    result_text = response['result']\n",
        "    return result_text\n",
        "\n",
        "pred = gr.Interface(\n",
        "    support_assist,\n",
        "    title = \"Radiantloom Support Assist\",\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Question\",\n",
        "            lines=1,\n",
        "            placeholder=\"What is Lodgify?\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs= 'text'\n",
        ")\n",
        "\n",
        "pred.launch(share=True)"
      ],
      "metadata": {
        "id": "vGeHB3JNEDZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_8tC4gwBGfqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
